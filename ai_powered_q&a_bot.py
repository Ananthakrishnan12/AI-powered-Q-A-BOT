# -*- coding: utf-8 -*-
"""AI Powered Q&A Bot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PlzxyIC8NFswETCrfboTelirCXf250PY

# Install Required Libaries:
"""

#!pip install langchain llama-cpp-python streamlit streamlit_chat langchain_community

"""# Download llama-2-7b model from Hugging face - The Bloke."""

!wget -O llama-2-7b.gguf https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf

"""# Load the Model with Streamlit UI:"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# from langchain.chains import LLMChain
# from langchain.prompts import PromptTemplate
# from langchain.llms import LlamaCpp
# 
# st.title("AI-Powered Q&A BOT")
# st.write("I am AI Assistant Alexa, You can Ask me Anything, and I will response with Possible reply.")
# 
# @st.cache_resource
# def load_model():
#   return LlamaCpp(
#       model_path="/content/llama-2-7b.gguf",
#       n_ctx=2048,
#       temperature=0.7,
#       top_p=0.9
#   )
# 
# llm=load_model()
# 
# # Prompt Template:
# template="""
# As AI Assitant: Answer the following questions...
# 
# Questions:{question}
# 
# Answer:
# """
# prompt=PromptTemplate(input_variables=["question"],template=template)
# chain=LLMChain(llm=llm,prompt=prompt)
# 
# 
# #chat History:
# if "messages" not in st.session_state:
#   st.session_state.messages=[]
# 
# for message in st.session_state.messages:
#   with st.chat_message(message["role"]):
#     st.write(message["content"])
# 
# 
# # User Input:
# user_input=st.text_input("Ask me Anything..",key="user_input")
# 
# if user_input:
#   response=chain.run({"question":user_input})
# 
#   st.session_state.messages.append({"role":"user","content":user_input})
#   st.session_state.messages.append({"role":"assistant","content":response})
# 
#   with st.chat_message("assistant"):
#     st.write(response)
# 
#

"""# Code for streamlit to run on cloud..."""

!pip install pyngrok --quiet
!ngrok authtoken
from pyngrok import ngrok
import os

# Kill any existing Streamlit/Ngrok instances
os.system("pkill -9 streamlit")
os.system("pkill -9 ngrok")

# Start Streamlit in the background
os.system("streamlit run app.py &")

# Connect ngrok to Streamlit
tunnel = ngrok.connect(8501, "http")
print("ðŸš€ Streamlit is running on:", tunnel.public_url)